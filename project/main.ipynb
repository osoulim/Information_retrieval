{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Obama-McCain 2008 debate tweet classification\n",
    "### Mohammadreza Osouli - 610395077\n",
    "\n",
    "This jupyter note-book compare different classifiers and word-vectorizing methods accuracy on classifing emotions of tweets.\n",
    "\n",
    "### Loding dataset\n",
    "First of all, we should read data from excel file. In the code bellow, the dataset coloumns text and target are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset(filename, cols):\n",
    "    dataset = pd.read_csv(filename, encoding='latin-1')\n",
    "    dataset.columns = cols\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(\"StrictOMD.csv\", ['target', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "Pre-processing is one of the most important parts in text-mining or text-classification tasks. In this task, I did the following edits to any tweet in dataset.\n",
    "1. Making the tweet lowercase\n",
    "2. Removing mentions and hashtags\n",
    "3. Removing punctuations (this part can be very tricky but in the models I used, I should do it)\n",
    "4. Removing stop-words (like the last part, this may be tricky at all, but should be done in this order)\n",
    "4. Stemming words (I did this part to only have the actual root of any word as result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_tweet_text(tweet):\n",
    "    tweet.lower()\n",
    "    # Remove urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'\\@\\w+|\\#\\w+', '', tweet)\n",
    "    # Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
    "\n",
    "    result = \" \".join(lemma_words)\n",
    "    # print(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "dataset.text = dataset['text'].apply(preprocess_tweet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature vectors\n",
    "\n",
    "I used two methods for getting feature vector for tweets, tf-idf and glove.\n",
    "\n",
    "For glove method, I used a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_feature_vector(train_fit):\n",
    "    vector = TfidfVectorizer(sublinear_tf=True)\n",
    "    vector.fit(train_fit)\n",
    "    return vector\n",
    "\n",
    "tf_vector = get_feature_vector(np.array(dataset.iloc[:, 1]).ravel())\n",
    "\n",
    "tf_X = tf_vector.transform(np.array(dataset.iloc[:, 1]).ravel())\n",
    "tf_y = np.array(dataset.iloc[:, 0]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from zeugma.embeddings import EmbeddingTransformer\n",
    "glove = EmbeddingTransformer('glove')\n",
    "glove_X = glove.transform(np.array(dataset.iloc[:, 1]).ravel())\n",
    "glove_y = np.array(dataset.iloc[:, 0]).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "As we didn't have test and train data separated, I used cross validation with 5 splits to evalute my models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers: 1. Naive-Bayes\n",
    "\n",
    "Naive-Bayes method used as first classifier. As it cannot handle negative values, I did'nt run it on glove features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model with tf-idf: [0.74545455 0.81818182 0.76363636 0.74909091 0.77818182]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_model = MultinomialNB()\n",
    "scores = cross_val_score(NB_model, tf_X, tf_y, cv=cv)\n",
    "print(\"Naive Bayes Model with tf-idf:\", scores)\n",
    "\n",
    "# scores = cross_val_score(NB_model, glove_X, glove_y, cv=cv)\n",
    "# print(\"Naive Bayes Model with glove:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers: 2. Logistic Regression\n",
    "\n",
    "Loggistic regression is always a choice for classifying large size feature vectors. As the result shown, It had better accuracy on tf-idf method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with tf_idf: [0.76727273 0.80363636 0.76       0.75272727 0.79636364]\n",
      "Logistic Regression with glove: [0.74181818 0.74909091 0.73090909 0.72       0.71636364]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR_model = LogisticRegression(solver='lbfgs')\n",
    "scores = cross_val_score(LR_model, tf_X, tf_y, cv=cv)\n",
    "print(\"Logistic Regression with tf_idf:\", scores)\n",
    "\n",
    "scores = cross_val_score(LR_model, glove_X, glove_y, cv=cv)\n",
    "print(\"Logistic Regression with glove:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifieres: 3. Support Vector Machine\n",
    "\n",
    "After loggistic reggresion, Support vector machines are always a powerfull choice for classifying well-separated features. It had the best score among all classifiers with tf-idf features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vector machine with tf_idf: [0.81090909 0.82545455 0.81454545 0.82181818 0.82909091]\n",
      "Support vector machine with glove: [0.73454545 0.74181818 0.72       0.70909091 0.72363636]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVC_model = SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(SVC_model, tf_X, tf_y, cv=cv)\n",
    "print(\"Support vector machine with tf_idf:\", scores)\n",
    "\n",
    "scores = cross_val_score(SVC_model, glove_X, glove_y, cv=cv)\n",
    "print(\"Support vector machine with glove:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers: 4. Multi-Layer Perceptron\n",
    "At last, I tried neural networks for classifying our vectors. As I guessed, features were not well separated, so I used MLP networks to have a better clustering. Its results on tf-idf feauters seems fine but the network can be tuned better in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Layer Perceptron with tf_idf: [0.81090909 0.8        0.83272727 0.85818182 0.81454545]\n",
      "Multi-Layer Perceptron with glove: [0.69454545 0.71272727 0.76727273 0.68727273 0.72363636]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP_model = MLPClassifier(solver='lbfgs', alpha=1e-4,\n",
    "                    hidden_layer_sizes=(1200, 200), random_state=1)\n",
    "scores = cross_val_score(MLP_model, tf_X, tf_y, cv=cv)\n",
    "print(\"Multi-Layer Perceptron with tf_idf:\", scores)\n",
    "\n",
    "scores = cross_val_score(MLP_model, glove_X, glove_y, cv=cv)\n",
    "print(\"Multi-Layer Perceptron with glove:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Four classifiers and two feature methods were implemented in this notebook to classify Obama-McCaine 2008 debate tweets' emotions. Best accuracy achived by using SVM on tf-idf vectors with about 81% accuracy. By contrast we have MLP on Glove vectors with about 68% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
